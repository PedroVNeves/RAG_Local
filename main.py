from langchain_chroma.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama.llms import OllamaLLM
import gradio as gr

# --- Configura√ß√µes do RAG ---
#Modelo escolhido para o embedding
model_name = "all-MiniLM-L6-v2"

#configura√ßa√µ do embedding
embedding_model = HuggingFaceEmbeddings(
    model_name=model_name,
    # O dispositivo 'cpu' garante que funcione em qualquer lugar
    model_kwargs={'device': 'cpu'}
)

#Diret√≥rio do DataBase
CAMINHO_DB = "db"

#Modelo do prompt
prompt_template = """
Responda a pergunta do usu√°rio:
{pergunta}

de forma curta e com base nessas informa√ß√µes:

{base_conhecimento}

Se voc√™ n√£o encontrar a resposta para a pergunta do usu√°rio nessas informa√ß√µes,
responda n√£o sei te dizer isso
"""

# Inicializa o modelo Ollama e o Banco de Dados Chroma fora da fun√ß√£o para efici√™ncia
try:
    # Tenta inicializar o LLM. Garanta que o Ollama esteja rodando e o modelo 'llama3' esteja instalado.
    modelo = OllamaLLM(model="llama3")
except Exception as e:
    print(f"AVISO: Falha ao inicializar Ollama. Verifique se o servidor est√° rodando e o modelo 'llama3' est√° instalado. Erro: {e}")
    # Se falhar, defina como None para evitar que a interface trave
    modelo = None

try:
    # Tenta carregar o DB. Garanta que o DB j√° foi criado com 'criar_db.py'.
    db = Chroma(persist_directory=CAMINHO_DB, embedding_function=embedding_model)
except Exception as e:
    print(f"AVISO: Falha ao carregar Chroma DB. Verifique se o diret√≥rio '{CAMINHO_DB}' existe. Erro: {e}")
    db = None

# ----------------------------------------------------------------------------------
# FUN√á√ÉO PRINCIPAL: Recebe a pergunta do Gradio e retorna a resposta
# ----------------------------------------------------------------------------------
def obter_resposta_rag(pergunta):
    if db is None:
        return "Erro: O banco de dados vetorial (Chroma DB) n√£o p√¥de ser carregado. Verifique os logs."
    if modelo is None:
        return "Erro: O modelo de Linguagem (Ollama LLM) n√£o p√¥de ser inicializado. Verifique os logs."

    # 1. Busca por similaridade
    # Abaixamos o k para 2 ou 3 para evitar contextos muito grandes
    resultados = db.similarity_search_with_relevance_scores(pergunta, k=3)

    # 2. Verifica√ß√£o de Relev√¢ncia
    LIMITE_RELEVANCIA = 0.6

    if not resultados or resultados[0][1] > LIMITE_RELEVANCIA:
        return "N√£o consegui encontrar uma resposta relevante na base de conhecimento. Tente reformular sua pergunta."

    # 3. Formata√ß√£o da Base de Conhecimento
    textos_resultado = []
    for resultado in resultados:
        # Adicionamos a pontua√ß√£o para fins de depura√ß√£o, mas n√£o no prompt final
        # print(f"Resultado encontrado com score: {resultado[1]}")
        textos_resultado.append(resultado[0].page_content)
    
    base_conhecimento = "\n\n----\n\n".join(textos_resultado)
    
    # 4. Constru√ß√£o do Prompt (usando o Template de Chat)
    prompt_final = ChatPromptTemplate.from_template(prompt_template)
    prompt_formatado = prompt_final.invoke({"pergunta": pergunta, "base_conhecimento": base_conhecimento})
    
    # 5. Invoca√ß√£o do Modelo
    # Convertemos o objeto 'prompt_formatado' para uma string para o modelo Ollama
    texto_resposta = modelo.invoke(prompt_formatado.to_string())
    
    return texto_resposta

# ----------------------------------------------------------------------------------
# INTERFACE GR√ÅFICA COM GRADIO
# ----------------------------------------------------------------------------------

# Cria√ß√£o dos componentes da interface
iface = gr.Interface(
    # A fun√ß√£o Python que ser√° chamada
    fn=obter_resposta_rag,
    
    # Componente de entrada (uma caixa de texto para a pergunta)
    inputs=gr.Textbox(
        lines=2, 
        placeholder="Digite sua pergunta aqui...", 
        label="Sua Pergunta"
    ),
    
    # Componente de sa√≠da (uma caixa de texto para a resposta da IA)
    outputs=gr.Textbox(
        lines=10, 
        label="Resposta da IA (RAG)"
    ),
    
    # T√≠tulo e descri√ß√£o da interface
    title="ü§ñ Sistema de Perguntas e Respostas (RAG) Local",
    description="Fa√ßa uma pergunta e a IA responder√° com base no conte√∫do da sua base de conhecimento (Arquivos PDF).",
    
    # Tema mais escuro para um visual mais moderno (opcional)
    theme=gr.themes.Soft() 
)

# Inicia a interface. Ela abrir√° automaticamente no seu navegador.
iface.launch(share=True)