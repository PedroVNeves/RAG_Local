markdown# ğŸ¦œ Sistema RAG (Retrieval Augmented Generation) Local com Llama 3

Este projeto implementa um Sistema de GeraÃ§Ã£o Aumentada por RecuperaÃ§Ã£o (RAG) para consulta a documentos, combinando a arquitetura robusta do LangChain com modelos de cÃ³digo aberto. O sistema utiliza a interface Gradio para permitir a interaÃ§Ã£o com o usuÃ¡rio.

**Foco do Projeto:** Garantir a mÃ¡xima **portabilidade** (funciona bem em CPU) e **velocidade de resposta** (minimizando o tempo de inferÃªncia do LLM).

## ğŸš€ Tecnologias Principais

* **LLM (Modelo de Linguagem):** [**Llama 3**](https://ollama.com/library/llama3) (Rodando localmente via **Ollama**)
* **Framework:** **LangChain** (para orquestraÃ§Ã£o RAG)
* **Embeddings:** **`all-MiniLM-L6-v2`** (via HuggingFace) â€“ Modelo ultrarrÃ¡pido, otimizado para execuÃ§Ã£o em CPU
* **Banco de Dados Vetorial:** **Chroma DB** (armazenamento local na pasta `db`)
* **Interface:** **Gradio**

## âš¡ OtimizaÃ§Ãµes para Velocidade (CPU-Only)

Para reduzir a latÃªncia de aproximadamente 20 segundos em ambientes com restriÃ§Ã£o de hardware, foram implementadas as seguintes otimizaÃ§Ãµes no cÃ³digo:

1. **Modelo LLM Leve:** Uso do Llama 3 (versÃ£o base ou quantizada) via Ollama
2. **Modelo de Embeddings RÃ¡pido:** Uso do `all-MiniLM-L6-v2`
3. **Contexto Reduzido:** A busca de documentos (`similarity_search`) foi limitada a **`k=2`**, enviando menos texto para o LLM processar e acelerando o tempo de geraÃ§Ã£o da resposta

## âš™ï¸ PrÃ©-requisitos

Para executar o sistema, vocÃª precisa ter:

1. **Python 3.9+** instalado
2. O software **[Ollama](https://ollama.com/)** instalado e o serviÃ§o **ativo** (rodando em segundo plano)

## ğŸ’¡ Como Executar o Projeto

### 1. Configurar o Ambiente Python
```bash
# Crie o ambiente virtual
python -m venv .venv

# Ative o ambiente virtual
# No Windows:
.venv\Scripts\activate
# No macOS/Linux:
source .venv/bin/activate

# Instale as dependÃªncias listadas no requirements.txt
pip install -r requirements.txt
```

### 2. Baixar o Modelo LLM (Llama 3)

O modelo deve ser instalado no Ollama antes de rodar o cÃ³digo Python:
```bash
ollama pull llama3
```

### 3. Executar o Script Principal

Certifique-se de que o Ollama estÃ¡ ativo e que o banco de dados vetorial (`db/`) jÃ¡ foi criado em uma etapa anterior (por exemplo, usando um script `criar_db.py` separado).

Com o ambiente virtual ativado, execute o script:
```bash
python rag_app.py
```

> **Nota:** Ajuste `rag_app.py` para o nome real do seu arquivo.

A interface Gradio serÃ¡ aberta automaticamente no seu navegador, permitindo que vocÃª interaja com o sistema RAG.

## ğŸ“ Estrutura do Projeto
```
.
â”œâ”€â”€ db/                  # Banco de dados vetorial Chroma
â”œâ”€â”€ rag_app.py          # Script principal da aplicaÃ§Ã£o
â”œâ”€â”€ criar_db.py         # Script para criar o banco de dados (opcional)
â”œâ”€â”€ requirements.txt    # DependÃªncias do projeto
â””â”€â”€ README.md          # Este arquivo
```

## ğŸ¤ ContribuiÃ§Ãµes

ContribuiÃ§Ãµes sÃ£o bem-vindas! Sinta-se Ã  vontade para abrir issues ou pull requests.

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Consulte o arquivo LICENSE para mais detalhes.